---
title: "Final Project"
author: "Grace DesJardins"
format: pdf
editor: visual
---

## Wikipedia Web Scraping

```{r, results='hide'}
library(xml2)
library(rvest)
library(tidyverse)
library(tidytext)
```

**October 6 to 12**

```{r}
# reading in the html page as an R object
url <- read_html("https://en.wikipedia.org/wiki/Wikipedia:Top_25_Report/October_6_to_12,_2024")
```

```{r}
# extracting the tables from url
table_oct6to12 <- url %>% 
  html_table(fill = TRUE)
```

```{r}
# using str() on tables (a list)
str(table_oct6to12)
```

```{r}
# extracting the table and saving it as "top_25_oct6to12"
top_25_oct6to12 <- table_oct6to12[[1]]
# printing the result
print(top_25_oct6to12)
```

```{r}
# cleaning out the rows with NA
top_25_oct6to12 <- top_25_oct6to12[, -c(3,5)]
top_25_oct6to12
```

**October 13 to 19**

```{r}
# reading in the html page as an R object
url2 <- read_html("https://en.wikipedia.org/wiki/Wikipedia:Top_25_Report/October_13_to_19,_2024")
```

```{r}
# extracting the tables from url
table_oct13to19 <- url2 %>% 
  html_table(fill = TRUE)
```

```{r}
# using str() on tables (a list)
str(table_oct13to19)
```

```{r}
# extracting the table and saving it as "top_25_oct13to19"
top_25_oct13to19 <- table_oct13to19[[1]]
# printing the result
print(top_25_oct13to19)
```

```{r}
# cleaning out the rows with NA
top_25_oct13to19 <- top_25_oct13to19[, -c(3,5)]
top_25_oct13to19
```

**October 20 to 26**

```{r}
# reading in the html page as an R object
url3 <- read_html("https://en.wikipedia.org/wiki/Wikipedia:Top_25_Report/October_20_to_26,_2024")
```

```{r}
# extracting the tables from url
table_oct20to26 <- url3 %>% 
  html_table(fill = TRUE)
```

```{r}
# using str() on tables (a list)
str(table_oct20to26)
```

```{r}
# extracting the table and saving it as "top_25_oct20to26"
top_25_oct20to26 <- table_oct20to26[[1]]
# printing the result
print(top_25_oct20to26)
```

```{r}
# cleaning out the rows with NA
top_25_oct20to26 <- top_25_oct20to26[, -c(3,5)]
top_25_oct20to26
```

**October 27 to November 2**

```{r}
# reading in the html page as an R object
url4 <- read_html("https://en.wikipedia.org/wiki/Wikipedia:Top_25_Report/October_27_to_November_2,_2024")
```

```{r}
# extracting the tables from url
table_oct27tonov2 <- url4 %>% 
  html_table(fill = TRUE)
```

```{r}
# using str() on tables (a list)
str(table_oct27tonov2)
```

```{r}
# extracting the table and saving it as "top_25_oct27tonov2"
top_25_oct27tonov2 <- table_oct27tonov2[[1]]
# printing the result
print(top_25_oct27tonov2)
```

```{r}
# cleaning out the rows with NA
top_25_oct27tonov2 <- top_25_oct27tonov2[, -c(3,5)]
top_25_oct27tonov2
```

**Combining the four tables**

```{r}
combined <- rbind(top_25_oct6to12, top_25_oct13to19, top_25_oct20to26, top_25_oct27tonov2)
combined <- combined[, -c(1,3)]
```

**Cleaning the data using tidytext**

```{r}
tokens <- combined %>%
  unnest_tokens(word, `Notes/about`)

data("stop_words")

tokens_cleaned <- tokens %>%
  anti_join(stop_words, by = "word")

print(tokens_cleaned)
```

**Finding the common words overall**

```{r}
# common words across all articles
common_words_overall <- tokens_cleaned %>%
  count(word, sort = TRUE)

print(common_words_overall)
```

```{r}
# common words by article (not super insightful since not many words with n>1)
common_words_by_article <- tokens_cleaned %>%
  count(Article, word, sort = TRUE) %>%  
  group_by(Article) %>%                  
  top_n(10) %>%                       
  arrange(Article, desc(n))            

print(common_words_by_article)
```

```{r}
# finding duplicate articles across weeks
duplicates <- combined %>%
  group_by(Article) %>%
  summarise(n = n(), .groups = "drop") %>%
  filter(n() > 1) %>%
  arrange(desc(n))

# printing the duplicates
print(duplicates)
```

```{r}
# common words by articles commonly searched all 4 weeks
common_words_by_article_4wks <- tokens_cleaned %>%
  count(Article, word, sort = TRUE) %>%  
 # group_by(Article) %>%  
  filter(Article == "Deaths in 2024" | Article == "Elon Musk" | Article == "Lyle and Erik Menendez") %>%
  top_n(10) %>%                       
  arrange(Article, desc(n))            

print(common_words_by_article_4wks)
```

## Google Trends API

```{r}
#| message = FALSE
library(tidyverse)
library(gtrendsR)
library(censusapi)
```

```{r, cache=TRUE}
res <- gtrends(c("death", "musk", "menendez"), 
               geo = "", 
               time = "2024-10-06 2024-11-02", 
               low_search_volume = TRUE)
plot(res)
```

```{r}
# transforming results into a tibble
results <- as_tibble(res$interest_over_time)

# creating data set called death, including "death" as the only keyword
death <- results %>%
  filter(keyword == "death")

# creating data set called musk, including "musk" as the only keyword
musk <- results %>%
  filter(keyword == "musk")

# creating data set called menendez, including "menendez" as the only keyword
menendez <- results %>%
  filter(keyword == "menendez")

# finding the mean of the search hits
mean(death$hits)
mean(musk$hits)
mean(menendez$hits)

# finding the median of the search hits
median(death$hits)
median(musk$hits)
median(menendez$hits)

# finding the variance of the search hits
var(death$hits)
var(musk$hits)
var(menendez$hits)
```
